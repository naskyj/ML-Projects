{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "f-_Wka-vo3MR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lime\n",
    "import shap\n",
    "import lime.lime_tabular\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>...</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>fa</th>\n",
       "      <th>fb</th>\n",
       "      <th>fc</th>\n",
       "      <th>fd</th>\n",
       "      <th>fe</th>\n",
       "      <th>ff</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 257 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  01  02  03  04  05  06  07  08  09  ...  f7  f8  f9  fa  fb  fc  fd  \\\n",
       "0   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
       "1   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
       "2   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
       "3   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
       "4   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
       "\n",
       "   fe  ff  Class  \n",
       "0   0   0      1  \n",
       "1   0   0      1  \n",
       "2   0   0      1  \n",
       "3   0   0      1  \n",
       "4   0   0      1  \n",
       "\n",
       "[5 rows x 257 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('xai.csv', index_col=False)\n",
    "df.columns = [i.zfill(2) for i in df.columns]\n",
    "df.drop(axis=1, labels = 'Unnamed: 0', inplace=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LmSq84xTzZkW",
    "outputId": "f71b4c98-43a2-4b65-f447-b9d7a23f53d3"
   },
   "outputs": [],
   "source": [
    "X = df.iloc[:, 0:256]\n",
    "y = df.Class\n",
    "\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "\n",
    "y = encoder.transform(y)\n",
    "y = np_utils.to_categorical(y, num_classes=14)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11a7fed29e0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(256,)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(14, activation='softmax')  ])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=50, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1C4Xju5azpqf",
    "outputId": "d491c456-22b7-4a52-e8c2-d608c5adbd94"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Permutation explainer: 3643it [28:09,  2.15it/s]                                                                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Features with Negative Impact on the Model:\n",
      "Feature 4 - 0.0059\n",
      "Feature 7 - 0.0052\n",
      "Feature 9 - 0.0046\n",
      "Feature 11 - 0.0041\n",
      "Feature 13 - 0.0022\n",
      "Feature 2 - 0.0019\n",
      "Feature 1 - 0.0015\n",
      "Feature 8 - 0.0014\n",
      "Feature 10 - 0.0012\n",
      "Feature 12 - 0.0011\n"
     ]
    }
   ],
   "source": [
    "explainer = shap.Explainer(model, X_train.values, algorithm=\"permutation\", max_evals=1000)\n",
    "\n",
    "shap_values = explainer.shap_values(X_test.values)\n",
    "\n",
    "mean_abs_shap_values = np.mean(np.abs(shap_values[0]), axis=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Features with Negative Impact on the Model:\n",
      "Feature 4 - 0.0059\n",
      "Feature 7 - 0.0052\n",
      "Feature 9 - 0.0046\n",
      "Feature 11 - 0.0041\n",
      "Feature 13 - 0.0022\n",
      "Feature 2 - 0.0019\n",
      "Feature 1 - 0.0015\n",
      "Feature 8 - 0.0014\n",
      "Feature 10 - 0.0012\n",
      "Feature 12 - 0.0011\n",
      "Feature 5 - 0.0010\n",
      "Feature 3 - 0.0009\n",
      "Feature 6 - 0.0005\n",
      "Feature 0 - 0.0004\n"
     ]
    }
   ],
   "source": [
    "sorted_feature_importances = sorted(enumerate(mean_abs_shap_values), key=lambda x: x[1], reverse=True)\n",
    "top_10_negative_features = sorted_feature_importances[:60]\n",
    "\n",
    "print(\"Top 10 Features with Negative Impact on the Model:\")\n",
    "for feature_idx, importance in top_10_negative_features:\n",
    "    print(f\"Feature {feature_idx} - {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "16HajYBUAqY6",
    "outputId": "91b2eaf3-81ce-4f05-b280-99a716273e8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Features with Negative Impact on the Model:\n",
      "Feature 1 - 0.1329\n",
      "Feature 0 - 0.0861\n",
      "Feature 2 - 0.0676\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load a sample dataset (Iris dataset)\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train a neural network model (you can replace this with your own model)\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(3, activation='softmax')  # 3 classes for Iris dataset\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=50, verbose=0)\n",
    "\n",
    "# Initialize the SHAP explainer with a higher max_evals value\n",
    "explainer = shap.Explainer(model, X_train, algorithm=\"permutation\", max_evals=1000)\n",
    "\n",
    "# Get SHAP values for all samples in the test set\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Calculate the mean absolute SHAP value for each feature\n",
    "mean_abs_shap_values = np.mean(np.abs(shap_values[0]), axis=0)  # Use shap_values[0] for the first class\n",
    "\n",
    "# Sort and print the top 10 features that negatively impact the model\n",
    "sorted_feature_importances = sorted(enumerate(mean_abs_shap_values), key=lambda x: x[1], reverse=True)\n",
    "top_10_negative_features = sorted_feature_importances[:10]\n",
    "\n",
    "print(\"Top 10 Features with Negative Impact on the Model:\")\n",
    "for feature_idx, importance in top_10_negative_features:\n",
    "    print(f\"Feature {feature_idx} - {importance:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZUHmuMLBjNp"
   },
   "outputs": [],
   "source": [
    "sample_idx = 1\n",
    "sample = X_test.values[sample_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DSP3iN7-3lbK",
    "outputId": "97ee9499-d952-43b2-bded-6930d4efa1a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, mode=\"classification\")\n",
    "explanation = explainer.explain_instance(sample, model.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ey-onXfD9ZPq",
    "outputId": "4d9c0110-c148-408c-c06f-9cf0fa7bbd2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Features with Negative Impact on the Model:\n",
      "Feature 0 - 0.5835\n",
      "Feature 1 - 0.5835\n",
      "Feature 2 - 0.5835\n",
      "Feature 3 - 0.5835\n",
      "Feature 4 - 0.5835\n",
      "Feature 5 - 0.5835\n",
      "Feature 6 - 0.5835\n",
      "Feature 7 - 0.5835\n",
      "Feature 8 - 0.5835\n",
      "Feature 9 - 0.5835\n",
      "Feature 10 - 0.5835\n",
      "Feature 11 - 0.5835\n",
      "Feature 12 - 0.5835\n",
      "Feature 13 - 0.5835\n",
      "Feature 14 - 0.5835\n",
      "Feature 15 - 0.5835\n",
      "Feature 16 - 0.5835\n",
      "Feature 17 - 0.5835\n",
      "Feature 18 - 0.5835\n",
      "Feature 19 - 0.5835\n",
      "Feature 20 - 0.5835\n",
      "Feature 21 - 0.5835\n",
      "Feature 22 - 0.5835\n",
      "Feature 23 - 0.5835\n",
      "Feature 24 - 0.5835\n",
      "Feature 25 - 0.5835\n",
      "Feature 26 - 0.5835\n",
      "Feature 27 - 0.5835\n",
      "Feature 28 - 0.5835\n",
      "Feature 29 - 0.5835\n",
      "Feature 30 - 0.5835\n",
      "Feature 31 - 0.5835\n",
      "Feature 32 - 0.5835\n",
      "Feature 33 - 0.5835\n",
      "Feature 34 - 0.5835\n",
      "Feature 35 - 0.5835\n",
      "Feature 36 - 0.5835\n",
      "Feature 37 - 0.5835\n",
      "Feature 38 - 0.5835\n",
      "Feature 39 - 0.5835\n",
      "Feature 40 - 0.5835\n",
      "Feature 41 - 0.5835\n",
      "Feature 42 - 0.5835\n",
      "Feature 43 - 0.5835\n",
      "Feature 44 - 0.5835\n",
      "Feature 45 - 0.5835\n",
      "Feature 46 - 0.5835\n",
      "Feature 47 - 0.5835\n",
      "Feature 48 - 0.5835\n",
      "Feature 49 - 0.5835\n",
      "Feature 50 - 0.5835\n",
      "Feature 51 - 0.5835\n",
      "Feature 52 - 0.5835\n",
      "Feature 53 - 0.5835\n",
      "Feature 54 - 0.5835\n",
      "Feature 55 - 0.5835\n",
      "Feature 56 - 0.5835\n",
      "Feature 57 - 0.5835\n",
      "Feature 58 - 0.5835\n",
      "Feature 59 - 0.5835\n",
      "Feature 60 - 0.5835\n",
      "Feature 61 - 0.5835\n",
      "Feature 62 - 0.5835\n",
      "Feature 63 - 0.5835\n",
      "Feature 64 - 0.5835\n",
      "Feature 65 - 0.5835\n",
      "Feature 66 - 0.5835\n",
      "Feature 67 - 0.5835\n",
      "Feature 68 - 0.5835\n",
      "Feature 69 - 0.5835\n",
      "Feature 70 - 0.5835\n",
      "Feature 71 - 0.5835\n",
      "Feature 72 - 0.5835\n",
      "Feature 73 - 0.5835\n",
      "Feature 74 - 0.5835\n",
      "Feature 75 - 0.5835\n",
      "Feature 76 - 0.5835\n",
      "Feature 77 - 0.5835\n",
      "Feature 78 - 0.5835\n",
      "Feature 79 - 0.5835\n",
      "Feature 80 - 0.5835\n",
      "Feature 81 - 0.5835\n",
      "Feature 82 - 0.5835\n",
      "Feature 83 - 0.5835\n",
      "Feature 84 - 0.5835\n",
      "Feature 85 - 0.5835\n",
      "Feature 86 - 0.5835\n",
      "Feature 87 - 0.5835\n",
      "Feature 88 - 0.5835\n",
      "Feature 89 - 0.5835\n",
      "Feature 90 - 0.5835\n",
      "Feature 91 - 0.5835\n",
      "Feature 92 - 0.5835\n",
      "Feature 93 - 0.5835\n",
      "Feature 94 - 0.5835\n",
      "Feature 95 - 0.5835\n",
      "Feature 96 - 0.5835\n",
      "Feature 97 - 0.5835\n",
      "Feature 98 - 0.5835\n",
      "Feature 99 - 0.5835\n",
      "Feature 100 - 0.5835\n",
      "Feature 101 - 0.5835\n",
      "Feature 102 - 0.5835\n",
      "Feature 103 - 0.5835\n",
      "Feature 104 - 0.5835\n",
      "Feature 105 - 0.5835\n",
      "Feature 106 - 0.5835\n",
      "Feature 107 - 0.5835\n",
      "Feature 108 - 0.5835\n",
      "Feature 109 - 0.5835\n",
      "Feature 110 - 0.5835\n",
      "Feature 111 - 0.5835\n",
      "Feature 112 - 0.5835\n",
      "Feature 113 - 0.5835\n",
      "Feature 114 - 0.5835\n",
      "Feature 115 - 0.5835\n",
      "Feature 116 - 0.5835\n",
      "Feature 117 - 0.5835\n",
      "Feature 118 - 0.5835\n",
      "Feature 119 - 0.5835\n",
      "Feature 120 - 0.5835\n",
      "Feature 121 - 0.5835\n",
      "Feature 122 - 0.5835\n",
      "Feature 123 - 0.5835\n",
      "Feature 124 - 0.5835\n",
      "Feature 125 - 0.5835\n",
      "Feature 126 - 0.5835\n",
      "Feature 127 - 0.5835\n",
      "Feature 128 - 0.5835\n",
      "Feature 129 - 0.5835\n",
      "Feature 130 - 0.5835\n",
      "Feature 131 - 0.5835\n",
      "Feature 132 - 0.5835\n",
      "Feature 133 - 0.5835\n",
      "Feature 134 - 0.5835\n",
      "Feature 135 - 0.5835\n",
      "Feature 136 - 0.5835\n",
      "Feature 137 - 0.5835\n",
      "Feature 138 - 0.5835\n",
      "Feature 139 - 0.5835\n",
      "Feature 140 - 0.5835\n",
      "Feature 141 - 0.5835\n",
      "Feature 142 - 0.5835\n",
      "Feature 143 - 0.5835\n",
      "Feature 144 - 0.5835\n",
      "Feature 145 - 0.5835\n",
      "Feature 146 - 0.5835\n",
      "Feature 147 - 0.5835\n",
      "Feature 148 - 0.5835\n",
      "Feature 149 - 0.5835\n",
      "Feature 150 - 0.5835\n",
      "Feature 151 - 0.5835\n",
      "Feature 152 - 0.5835\n",
      "Feature 153 - 0.5835\n",
      "Feature 154 - 0.5835\n",
      "Feature 155 - 0.5835\n",
      "Feature 156 - 0.5835\n",
      "Feature 157 - 0.5835\n",
      "Feature 158 - 0.5835\n",
      "Feature 159 - 0.5835\n",
      "Feature 160 - 0.5835\n",
      "Feature 161 - 0.5835\n",
      "Feature 162 - 0.5835\n",
      "Feature 163 - 0.5835\n",
      "Feature 164 - 0.5835\n",
      "Feature 165 - 0.5835\n",
      "Feature 166 - 0.5835\n",
      "Feature 167 - 0.5835\n",
      "Feature 168 - 0.5835\n",
      "Feature 169 - 0.5835\n",
      "Feature 170 - 0.5835\n",
      "Feature 171 - 0.5835\n",
      "Feature 172 - 0.5835\n",
      "Feature 173 - 0.5835\n",
      "Feature 174 - 0.5835\n",
      "Feature 175 - 0.5835\n",
      "Feature 176 - 0.5835\n",
      "Feature 177 - 0.5835\n",
      "Feature 178 - 0.5835\n",
      "Feature 179 - 0.5835\n",
      "Feature 180 - 0.5835\n",
      "Feature 181 - 0.5835\n",
      "Feature 182 - 0.5835\n",
      "Feature 183 - 0.5835\n",
      "Feature 184 - 0.5835\n",
      "Feature 185 - 0.5835\n",
      "Feature 186 - 0.5835\n",
      "Feature 187 - 0.5835\n",
      "Feature 188 - 0.5835\n",
      "Feature 189 - 0.5835\n",
      "Feature 190 - 0.5835\n",
      "Feature 191 - 0.5835\n",
      "Feature 192 - 0.5835\n",
      "Feature 193 - 0.5835\n",
      "Feature 194 - 0.5835\n",
      "Feature 195 - 0.5835\n",
      "Feature 196 - 0.5835\n",
      "Feature 197 - 0.5835\n",
      "Feature 198 - 0.5835\n",
      "Feature 199 - 0.5835\n",
      "Feature 200 - 0.5835\n",
      "Feature 201 - 0.5835\n",
      "Feature 202 - 0.5835\n",
      "Feature 203 - 0.5835\n",
      "Feature 204 - 0.5835\n",
      "Feature 205 - 0.5835\n",
      "Feature 206 - 0.5835\n",
      "Feature 207 - 0.5835\n",
      "Feature 208 - 0.5835\n",
      "Feature 209 - 0.5835\n",
      "Feature 210 - 0.5835\n",
      "Feature 211 - 0.5835\n",
      "Feature 212 - 0.5835\n",
      "Feature 213 - 0.5835\n",
      "Feature 214 - 0.5835\n",
      "Feature 215 - 0.5835\n",
      "Feature 216 - 0.5835\n",
      "Feature 217 - 0.5835\n",
      "Feature 218 - 0.5835\n",
      "Feature 219 - 0.5835\n",
      "Feature 220 - 0.5835\n",
      "Feature 221 - 0.5835\n",
      "Feature 222 - 0.5835\n",
      "Feature 223 - 0.5835\n",
      "Feature 224 - 0.5835\n",
      "Feature 225 - 0.5835\n",
      "Feature 226 - 0.5835\n",
      "Feature 227 - 0.5835\n",
      "Feature 228 - 0.5835\n",
      "Feature 229 - 0.5835\n",
      "Feature 230 - 0.5835\n",
      "Feature 231 - 0.5835\n",
      "Feature 232 - 0.5835\n",
      "Feature 233 - 0.5835\n",
      "Feature 234 - 0.5835\n",
      "Feature 235 - 0.5835\n",
      "Feature 236 - 0.5835\n",
      "Feature 237 - 0.5835\n",
      "Feature 238 - 0.5835\n",
      "Feature 239 - 0.5835\n",
      "Feature 240 - 0.5835\n",
      "Feature 241 - 0.5835\n",
      "Feature 242 - 0.5835\n",
      "Feature 243 - 0.5835\n",
      "Feature 244 - 0.5835\n",
      "Feature 245 - 0.5835\n",
      "Feature 246 - 0.5835\n",
      "Feature 247 - 0.5835\n",
      "Feature 248 - 0.5835\n",
      "Feature 249 - 0.5835\n",
      "Feature 250 - 0.5835\n",
      "Feature 251 - 0.5835\n",
      "Feature 252 - 0.5835\n",
      "Feature 253 - 0.5835\n",
      "Feature 254 - 0.5835\n",
      "Feature 255 - 0.5835\n"
     ]
    }
   ],
   "source": [
    "aggregate_importances = np.zeros(X_train.shape[1])\n",
    "\n",
    "for class_index in range(num_classes):\n",
    "    try:\n",
    "        # Get feature importances for the class (if available)\n",
    "        class_importances = explanation.as_list(label=class_index)\n",
    "        # Aggregate feature importances for the class\n",
    "        if class_importances:\n",
    "            for _, importance in class_importances:\n",
    "                # Aggregate importance values directly\n",
    "                aggregate_importances += np.abs(importance)\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "# Sort and print the top 10 features that negatively impact the model\n",
    "sorted_feature_importances = sorted(enumerate(aggregate_importances), key=lambda x: x[1], reverse=True)\n",
    "top_10_negative_features = sorted_feature_importances[:300]\n",
    "\n",
    "print(\"Top 10 Features with Negative Impact on the Model:\")\n",
    "for feature_idx, importance in top_10_negative_features:\n",
    "    print(f\"Feature {feature_idx} - {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pZ5xhOaY338D"
   },
   "outputs": [],
   "source": [
    "lime_feature_importances = explanation.as_map()[1]\n",
    "sorted_feature_importances = sorted(lime_feature_importances, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aFjx6F7s4d2L"
   },
   "outputs": [],
   "source": [
    "aggregate_importances = np.zeros(X_train.shape[1])\n",
    "sorted_feature_importances = sorted(enumerate(aggregate_importances), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vHgyqoSS5Nfr"
   },
   "outputs": [],
   "source": [
    "negative_features = sorted_feature_importances[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ULApBIT39NRT",
    "outputId": "b4f5ac17-df8b-4e49-acf3-4fae012c39b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Features with Negative Impact on the Model:\n",
      "Feature 0 - 0.0000\n",
      "Feature 1 - 0.0000\n",
      "Feature 2 - 0.0000\n",
      "Feature 3 - 0.0000\n",
      "Feature 4 - 0.0000\n",
      "Feature 5 - 0.0000\n",
      "Feature 6 - 0.0000\n",
      "Feature 7 - 0.0000\n",
      "Feature 8 - 0.0000\n",
      "Feature 9 - 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 10 Features with Negative Impact on the Model:\")\n",
    "for feature_idx, importance in negative_features:\n",
    "    print(f\"Feature {feature_idx} - {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "id": "5ijqLYwn5jcp",
    "outputId": "ebfc2917-ce3c-4666-c4a8-a8ddda220f10"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-aa18e3be1ef9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimportance\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnegative_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{df.feature_names[feature]}-{importance: .4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5900\u001b[0m         ):\n\u001b[1;32m   5901\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5902\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5904\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'feature_names'"
     ]
    }
   ],
   "source": [
    "for feature, importance in negative_features:\n",
    "  print(f\"{df.feature_names[feature]}-{importance: .4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUcuZyB12_n4"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "Zuzo0yzerVsX",
    "outputId": "32f13f60-a2ff-413b-82f0-09349dc2e5f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3bc2ea04aa6b>\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclass_importances\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimportance\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclass_importances\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mfeature_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Extract the feature index provided by LIME\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                 \u001b[0maggregate_importances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mimportance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '4.25'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load a sample dataset (Iris dataset)\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train a neural network model (you can replace this with your own model)\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(3, activation='softmax')  # 3 classes for Iris dataset\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=50, verbose=0)\n",
    "\n",
    "# Initialize the LIME explainer\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train, mode=\"classification\")\n",
    "\n",
    "# Select a sample from the test data for explanation\n",
    "sample_idx = 0\n",
    "sample = X_test[sample_idx]\n",
    "\n",
    "# Initialize an array to store aggregate importances\n",
    "aggregate_importances = np.zeros(X_train.shape[1])\n",
    "\n",
    "# Loop through class indices and aggregate feature importances\n",
    "for class_index in range(len(data.target_names)):\n",
    "    try:\n",
    "        # Explain the model's prediction for the selected sample and class\n",
    "        explanation = explainer.explain_instance(sample, model.predict, labels=[class_index])\n",
    "        # Get feature importances for the class (if available)\n",
    "        class_importances = explanation.as_list(label=class_index)\n",
    "        # Aggregate feature importances for the class\n",
    "        if class_importances:\n",
    "            for feature, importance in class_importances:\n",
    "                feature_index = int(feature.split()[0])  # Extract the feature index provided by LIME\n",
    "                aggregate_importances[feature_index] += importance\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "# Sort and print the top 10 features that negatively impact the model\n",
    "sorted_feature_importances = sorted(enumerate(aggregate_importances), key=lambda x: x[1])\n",
    "top_10_negative_features = sorted_feature_importances[:10]\n",
    "\n",
    "print(\"Top 10 Features with Negative Impact on the Model:\")\n",
    "for feature_idx, importance in top_10_negative_features:\n",
    "    print(f\"Feature {feature_idx} - {importance:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
