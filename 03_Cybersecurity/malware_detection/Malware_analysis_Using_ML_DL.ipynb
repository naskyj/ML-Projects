{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-qmGnrREdo3"
   },
   "source": [
    "Import required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "iSr76h1OEPk-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import shutil\n",
    "\n",
    "# importing necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn import preprocessing, model_selection\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "\n",
    "\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense \n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnyJr6OMvizq"
   },
   "source": [
    "**Feature Exxtraction class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "VFxbef3i8uB6"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from pyparsing import Word, hexnums, WordEnd, Optional, alphas, alphanums\n",
    "\n",
    "\n",
    "class FeatureExtraction:\n",
    "    def __init__(self, trainLabelsCSVPath):\n",
    "        self.classes = pd.read_csv(trainLabelsCSVPath)\n",
    "\n",
    "  #returns a data fram containing the frequency of the hexadecimal in the file\n",
    "    def feature_extraction_bytes(self, folder_path):\n",
    "        total_dict = []\n",
    "        #loops through a list of all files in the folder\n",
    "        for file in os.listdir(folder_path):\n",
    "          #checks if the file has a byte extension\n",
    "          #processes only byte files\n",
    "            if file.find('bytes') != -1:\n",
    "                # print('current file', file)\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "                # get unique count\n",
    "                uniq = {}\n",
    "                #updates the dictionary uniq with the hexcode as keys and their frequency as values\n",
    "                #reads the byte file\n",
    "                with open(file_path) as f:\n",
    "                    lines = f.read().splitlines()\n",
    "                    f.close()\n",
    "                    for line in lines:\n",
    "                      #split each line by space\n",
    "                      #the first item is ignored since it is not a hexadecimal\n",
    "                        byt = line.split(' ')[1:]\n",
    "                        #looping through the hexadecimals\n",
    "                        for hexa in byt:\n",
    "                            if len(hexa) > 1:\n",
    "                                if hexa in uniq:\n",
    "                                    uniq[hexa] += 1\n",
    "                                else:\n",
    "                                    uniq[hexa] = 1\n",
    "\n",
    "                # get the class value \n",
    "                label = self.classes.loc[self.classes['Id'] == file.split('.')[0]]['Class'].values[0]\n",
    "                uniq['Class'] = label\n",
    "                # print(len(uniq))\n",
    "                # print(uniq)\n",
    "                total_dict.append(uniq)\n",
    "\n",
    "                data = pd.DataFrame(total_dict)\n",
    "        return data\n",
    "\n",
    "    # extract opcode\n",
    "\n",
    "    def get_opcode_count(self, file_path):\n",
    "        opcode_dic = {}\n",
    "\n",
    "        hex_integer = Word(hexnums) + WordEnd()  # use WordEnd to avoid parsing leading a-f of non-hex numbers as a hex\n",
    "        line = \".text:\" + hex_integer + Optional(\n",
    "            (hex_integer * (1,))(\"instructions\") + Word(alphas, alphanums)(\"opcode\"))\n",
    "\n",
    "        f = open(file_path, 'r', encoding='latin-1')\n",
    "        source = f.read().splitlines()\n",
    "        f.close()\n",
    "\n",
    "        for source_line in source:\n",
    "            try:\n",
    "                if '.text:' in source_line:\n",
    "                    result = line.parseString(source_line)\n",
    "                    if \"opcode\" in result:\n",
    "                        if result.opcode in opcode_dic:\n",
    "                            opcode_dic[result.opcode] += 1\n",
    "                        else:\n",
    "                            opcode_dic[result.opcode] = 1\n",
    "                    # print(result.opcode, result.instructions.asList())\n",
    "            except:\n",
    "                continue\n",
    "        return opcode_dic\n",
    "\n",
    "    # get section count\n",
    "    def get_section_count(self, file_path):\n",
    "        section_dic = {}\n",
    "\n",
    "        line = \".\" + Word(alphas, alphanums)(\"section\")\n",
    "\n",
    "        f = open(file_path, 'r', encoding='latin-1')\n",
    "        source = f.read().splitlines()\n",
    "        f.close()\n",
    "\n",
    "        for source_line in source:\n",
    "            try:\n",
    "                result = line.parseString(source_line)\n",
    "                if \"section\" in result:\n",
    "                    if result.section in section_dic:\n",
    "                        section_dic[result.section] += 1\n",
    "                    else:\n",
    "                        section_dic[result.section] = 1\n",
    "                    # print('section', result.section)\n",
    "            except:\n",
    "                continue\n",
    "        return section_dic\n",
    "\n",
    "    def feature_extraction_opcode(self, folder_path):\n",
    "        total_dict = []\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.find('asm') != -1:\n",
    "                # print('current file', file)\n",
    "                # get unique count\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "                uniq = self.get_opcode_count(file_path)\n",
    "\n",
    "                # get class value\n",
    "                label = self.classes.loc[self.classes['Id'] == file.split('.')[0]]['Class'].values[0]\n",
    "                uniq['Class'] = label\n",
    "                # print(len(uniq))\n",
    "                # print(uniq)\n",
    "                total_dict.append(uniq)\n",
    "\n",
    "                data = pd.DataFrame(total_dict)\n",
    "        return data\n",
    "\n",
    "    def feature_extraction_section(self, folder_path):\n",
    "        total_dict = []\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.find('asm') != -1:\n",
    "                # print('current file', file)\n",
    "                # get unique count\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "                uniq = self.get_section_count(file_path)\n",
    "\n",
    "                # get class value\n",
    "                label = self.classes.loc[self.classes['Id'] == file.split('.')[0]]['Class'].values[0]\n",
    "                uniq['Class'] = label\n",
    "                # print(len(uniq))\n",
    "                # print(uniq)\n",
    "                total_dict.append(uniq)\n",
    "\n",
    "                data = pd.DataFrame(total_dict)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJ7C2dDE4Yje"
   },
   "source": [
    "**MODEL CLASS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "alRcXqdGU4HF"
   },
   "outputs": [],
   "source": [
    "#ignore warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "#import the necessary libraries\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Reshape, Dropout\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Model:\n",
    "    def __init_subclass__(cls) -> None:\n",
    "        pass\n",
    "\n",
    "    def decisionTree(self, X_train, y_train, X_test, y_test):\n",
    "        # training a DescisionTreeClassifier\n",
    "        print('DescisionTreeClassifier')\n",
    "\n",
    "        # dtree_model = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3).fit(X_train, y_train)\n",
    "        dtree_model = DecisionTreeClassifier(criterion=\"gini\", max_depth=3).fit(X_train, y_train)\n",
    "        dtree_predictions = dtree_model.predict(X_test)\n",
    "\n",
    "        # Metrics evaluation\n",
    "        accuracy = metrics.accuracy_score(y_test, dtree_predictions)\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        precision, recall, fscore, _ = metrics.precision_recall_fscore_support(y_test, dtree_predictions,\n",
    "                                                                               average='weighted')\n",
    "        cm = confusion_matrix(y_test, dtree_predictions)\n",
    "        print(cm)\n",
    "\n",
    "        print(\"precision_recall_fscore_support:\", precision)\n",
    "        print(\"tpr = recall:\", recall)\n",
    "        print(\"fscore:\", fscore)\n",
    "        fpr = self.calculate_tpr_fpr(y_test, dtree_predictions)\n",
    "        return accuracy, precision, recall, fscore, fpr\n",
    "\n",
    "    # training a linear SVM classifier\n",
    "    def SVClassifier(self, X_train, y_train, X_test, y_test):\n",
    "        print('SVC Classifier')\n",
    "        svm_model_linear = SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    "        svm_predictions = svm_model_linear.predict(X_test)\n",
    "\n",
    "        # creating a confusion matrix\n",
    "        # cm = confusion_matrix(y_test, svm_predictions)\n",
    "\n",
    "        # Metrics evaluation\n",
    "        accuracy = metrics.accuracy_score(y_test, svm_predictions)\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        precision, recall, fscore, _ = metrics.precision_recall_fscore_support(y_test, svm_predictions,\n",
    "                                                                               average='weighted')\n",
    "        \n",
    "        cm = confusion_matrix(y_test, svm_predictions)\n",
    "        print(cm)\n",
    "        \n",
    "        print(\"precision_recall_fscore_support:\", precision)\n",
    "        print(\"recall:\", recall)\n",
    "        print(\"fscore:\", fscore)\n",
    "        fpr = self.calculate_tpr_fpr(y_test, svm_predictions)\n",
    "        return accuracy, precision, recall, fscore, fpr\n",
    "\n",
    "    # training a KNN classifier\n",
    "    def KNearestClassifier(self, X_train, y_train, X_test, y_test):\n",
    "        print('KNearestClassifier')\n",
    "        knn = KNeighborsClassifier(n_neighbors=7).fit(X_train, y_train)\n",
    "\n",
    "        # prediction matrix\n",
    "        knn_predictions = knn.predict(X_test)\n",
    "\n",
    "        # Metrics evaluation\n",
    "        accuracy = metrics.accuracy_score(y_test, knn_predictions)\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        precision, recall, fscore, _ = metrics.precision_recall_fscore_support(y_test, knn_predictions,\n",
    "                                                                               average='weighted')\n",
    "        cm = confusion_matrix(y_test, knn_predictions)\n",
    "        print(cm)\n",
    "        \n",
    "        print(\"precision_recall_fscore_support:\", precision)\n",
    "        print(\"recall:\", recall)\n",
    "        print(\"fscore:\", fscore)\n",
    "        fpr = self.calculate_tpr_fpr(y_test, knn_predictions)\n",
    "        return accuracy, precision, recall, fscore, fpr\n",
    "\n",
    "    # training a Naive Bayes classifier\n",
    "    def NaiveBayes(self, X_train, y_train, X_test, y_test):\n",
    "        print('NaiveBayes')\n",
    "        gnb = GaussianNB().fit(X_train, y_train)\n",
    "        gnb_predictions = gnb.predict(X_test)\n",
    "\n",
    "        # accuracy on X_test\n",
    "        accuracy = gnb.score(X_test, y_test)\n",
    "        print(accuracy)\n",
    "\n",
    "        # creating a confusion matrix\n",
    "        cm = confusion_matrix(y_test, gnb_predictions)\n",
    "\n",
    "        # Metrics evaluation\n",
    "        accuracy = metrics.accuracy_score(y_test, gnb_predictions)\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        precision, recall, fscore, _ = metrics.precision_recall_fscore_support(y_test, gnb_predictions,\n",
    "                                                                               average='weighted')\n",
    "        cm = confusion_matrix(y_test, gnb_predictions)\n",
    "        print(cm)\n",
    "        \n",
    "        print(\"precision_recall_fscore_support:\", precision)\n",
    "        print(\"recall:\", recall)\n",
    "        print(\"fscore:\", fscore)\n",
    "        fpr = self.calculate_tpr_fpr(y_test, gnb_predictions)\n",
    "        return accuracy, precision, recall, fscore, fpr\n",
    "\n",
    "    def SGD(self, X_train, y_train, X_test, y_test):\n",
    "        print('SGD')\n",
    "        clf = SGDClassifier(alpha=0.001, max_iter=100).fit(X_train, y_train)\n",
    "        clf_predictions = clf.predict(X_test)\n",
    "\n",
    "        # Model evaluation\n",
    "        accuracy = metrics.accuracy_score(y_test, clf_predictions)\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        precision, recall, fscore, _ = metrics.precision_recall_fscore_support(y_test, clf_predictions,\n",
    "                                                                               average='weighted')\n",
    "        cm = confusion_matrix(y_test, clf_predictions)\n",
    "        print(cm)\n",
    "        \n",
    "        print(\"precision_recall_fscore_support:\", precision)\n",
    "        print(\"recall:\", recall)\n",
    "        print(\"fscore:\", fscore)\n",
    "        fpr = self.calculate_tpr_fpr(y_test, clf_predictions)\n",
    "        return accuracy, precision, recall, fscore, fpr\n",
    "\n",
    "    def LogisticReg(self, X_train, y_train, X_test, y_test):\n",
    "        print('Logistic Regression')\n",
    "        clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "        clf_predictions = clf.predict(X_test)\n",
    "\n",
    "        # Model evaluation\n",
    "        accuracy = metrics.accuracy_score(y_test, clf_predictions)\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        precision, recall, fscore, _ = metrics.precision_recall_fscore_support(y_test, clf_predictions,\n",
    "                                                                               average='weighted')\n",
    "        cm = confusion_matrix(y_test, clf_predictions)\n",
    "        print(cm)\n",
    "        \n",
    "        print(\"precision_recall_fscore_support:\", precision)\n",
    "        print(\"recall:\", recall)\n",
    "        print(\"fscore:\", fscore)\n",
    "        fpr = self.calculate_tpr_fpr(y_test, clf_predictions)\n",
    "        return accuracy, precision, recall, fscore, fpr\n",
    "\n",
    "    def RandomForest(self, X_train, y_train, X_test, y_test):\n",
    "        print('RandomForest')\n",
    "        clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "        clf.fit(X_train, y_train)\n",
    "        clf_predictions = clf.predict(X_test)\n",
    "\n",
    "        # Model evaluation\n",
    "        accuracy = metrics.accuracy_score(y_test, clf_predictions)\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        precision, recall, fscore, _ = metrics.precision_recall_fscore_support(y_test, clf_predictions,\n",
    "                                                                               average='weighted')\n",
    "        cm = confusion_matrix(y_test, clf_predictions)\n",
    "        print(cm)\n",
    "        \n",
    "        print(\"precision_recall_fscore_support:\", precision)\n",
    "        print(\"recall:\", recall)\n",
    "        print(\"fscore:\", fscore)\n",
    "        fpr = self.calculate_tpr_fpr(y_test, clf_predictions)\n",
    "        return accuracy, precision, recall, fscore, fpr\n",
    "\n",
    "    def DnnPreprocessing(self, X, Y):\n",
    "        X = np.array(X)\n",
    "\n",
    "        # Transform name species into numerical values\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(Y)\n",
    "        Y = encoder.transform(Y)\n",
    "        Y = np_utils.to_categorical(Y)\n",
    "        \n",
    "        print(\"Before oversampling\",  X.shape, Y.shape)\n",
    "        \n",
    "        oversample = RandomOverSampler(sampling_strategy=\"not majority\")  #SMOTE()\n",
    "        \n",
    "        X, Y = oversample.fit_resample(X, Y)\n",
    "        \n",
    "        print(\"After oversampling\",  X.shape, Y.shape)\n",
    "\n",
    "        # split the data into train and test\n",
    "        train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size=0.1, random_state=0)\n",
    "        return train_x, test_x, train_y, test_y, encoder\n",
    "    \n",
    "    \n",
    "\n",
    "    def DNN_Model(self, train_x, test_x, train_y, test_y, epoch, encoder):\n",
    "        \n",
    "        print(\"dim: \", train_x.shape[1], train_y.shape[1])\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(30, input_dim=train_x.shape[1], activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(15, activation = \"relu\"))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(9, activation='softmax'))\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        history = model.fit(train_x, train_y, epochs=epoch, batch_size=100, validation_data=(test_x, test_y))\n",
    "\n",
    "        scores = model.evaluate(test_x, test_y)\n",
    "        print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))\n",
    "\n",
    "        predictions = model.predict(test_x)\n",
    "        prediction_ = np.argmax(predictions, axis=1)\n",
    "        prediction_ = encoder.inverse_transform(prediction_)\n",
    "\n",
    "        test_y_ = np.argmax(test_y, axis=1)\n",
    "        test_y_ = encoder.inverse_transform(test_y_)\n",
    "\n",
    "        # Model evaluation\n",
    "        accuracy = metrics.accuracy_score(test_y_, prediction_)\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        precision, recall, fscore, _ = metrics.precision_recall_fscore_support(test_y_, prediction_, average='weighted')\n",
    "        print(\"precision_recall_fscore_support:\", precision)\n",
    "        print(\"recall:\", recall)\n",
    "        print(\"fscore:\", fscore)\n",
    "        self.plot_Acc(history, epoch)\n",
    "        fpr = self.calculate_tpr_fpr(test_y_, prediction_)\n",
    "        return accuracy, precision, recall, fscore, fpr\n",
    "\n",
    "    def plot_Acc(self, history, epoch):\n",
    "        acc = history.history['accuracy']\n",
    "        val_acc = history.history['val_accuracy']\n",
    "\n",
    "        loss = history.history['loss']\n",
    "        val_loss = history.history['val_loss']\n",
    "\n",
    "        epochs_range = range(epoch)\n",
    "\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "        plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.title('Training and Validation Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs_range, loss, label='Training Loss')\n",
    "        plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.show()\n",
    "\n",
    "    def calculate_tpr_fpr(self, true_y, pred_y):\n",
    "        # creating a confusion matrix\n",
    "        confusion_matrix = metrics.confusion_matrix(true_y, pred_y)\n",
    "\n",
    "        FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "        FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "        TP = np.diag(confusion_matrix)\n",
    "        TN = confusion_matrix.sum() - (FP + FN + TP)\n",
    "        FP = FP.astype(float)\n",
    "        FN = FN.astype(float)\n",
    "        TP = TP.astype(float)\n",
    "        TN = TN.astype(float)\n",
    "        # Sensitivity, hit rate, recall, or true positive rate\n",
    "        # TPR = TP / (TP + FN)\n",
    "        # Fall out or false positive rate\n",
    "        FPR = FP / (FP + TN)\n",
    "        print('FPR', np.average(FPR))\n",
    "        return np.average(FPR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mM2kbG8Utox4"
   },
   "source": [
    "**Main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "XdIMxC1JwF_k"
   },
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np # linear algebra\n",
    "\n",
    "# from feature_extractor import FeatureExtraction\n",
    "# from models import Model\n",
    "\n",
    "data_path = \"malware-classification\"\n",
    "\n",
    "\n",
    "trainLabel_path = os.path.join(data_path, 'trainLabels.csv')\n",
    "MalwaDataPath = os.path.join(data_path, 'bytes')\n",
    "source = os.path.join(data_path, 'asmFiles')\n",
    "\n",
    "#trainLabel_path = 'trainLabels.csv'\n",
    "#MalwaDataPath = r'C:\\Users\\georg\\OneDrive\\Desktop\\Malware Classification - deep learning\\dataSample'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"malware-classification\"\n",
    "\n",
    "#source = os.path.join(data_path, 'train') \n",
    "byte_destination = os.path.join(data_path, 'byteFiles')\n",
    "source = os.path.join(data_path, 'asmFiles')\n",
    "\n",
    "\n",
    "if not os.path.isdir(byte_destination):\n",
    "    os.makedirs(byte_destination)\n",
    "\n",
    "if os.path.isdir(source):\n",
    "    #os.rename(source, asm_destination)\n",
    "    #source = asm_destination\n",
    "    all_files = os.listdir(source)\n",
    "    for file in all_files:\n",
    "        if (file.endswith(\"bytes\")):\n",
    "            shutil.move( os.path.join(asm_destination, file), byte_destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(byte_destination)\n",
    "files2 = os.listdir(source)\n",
    "\n",
    "for file in files[:5857]:\n",
    "    os.remove(os.path.join(byte_destination, file))\n",
    "    \n",
    "for file in files2[:5857]:\n",
    "    os.remove(os.path.join(source, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        00      97      FF      C6      49      C3      63      C0      74  \\\n",
      "0  35208.0  1545.0  5996.0   508.0   873.0   741.0   383.0  1056.0  1803.0   \n",
      "1  11674.0  3383.0  6864.0  3243.0  3185.0  3169.0  3417.0  3329.0  3402.0   \n",
      "2  21075.0   510.0  8366.0   586.0   689.0   669.0   430.0  1153.0  1843.0   \n",
      "3  21689.0    63.0  8769.0  1962.0   305.0  2224.0   432.0  1080.0  1820.0   \n",
      "4   5743.0  3022.0  3163.0  3006.0  2973.0  3144.0  2977.0  3031.0  3053.0   \n",
      "\n",
      "       59  ...      4B      DF      B1      EA      9D      A3      DE  \\\n",
      "0   958.0  ...   311.0   285.0   282.0   315.0   281.0   244.0   274.0   \n",
      "1  3327.0  ...  3275.0  3244.0  3248.0  3221.0  3213.0  3187.0  3134.0   \n",
      "2   506.0  ...   386.0   320.0   304.0   328.0   502.0   375.0   335.0   \n",
      "3   925.0  ...   198.0   884.0    62.0  2462.0   858.0   141.0   124.0   \n",
      "4  3081.0  ...  3044.0  3043.0  3009.0  3055.0  3034.0  3083.0  3058.0   \n",
      "\n",
      "       B9         ??  Class  \n",
      "0   274.0      364.0      9  \n",
      "1  3281.0  1491612.0      3  \n",
      "2   310.0     8012.0      1  \n",
      "3    95.0    20448.0      2  \n",
      "4  3010.0  1549980.0      3  \n",
      "\n",
      "[5 rows x 258 columns]\n"
     ]
    }
   ],
   "source": [
    "FE = FeatureExtraction(trainLabel_path)\n",
    "\n",
    "data_byte = FE.feature_extraction_bytes(byte_destination)\n",
    "\n",
    "data_byte.to_csv('file_byte.csv', encoding='utf-8', index=False)\n",
    "print (data_byte.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      text  idata    rdata      data  Class  rsrc  reloc  rata  bss  code  \\\n",
      "0  10290.0  241.0   1265.0     495.0      9   NaN    NaN   NaN  NaN   NaN   \n",
      "1   1694.0  126.0    211.0    1069.0      3   3.0    NaN   NaN  NaN   NaN   \n",
      "2  31295.0  379.0      NaN    1029.0      1   3.0    3.0   NaN  NaN   NaN   \n",
      "3  91180.0  507.0  21277.0    6551.0      2   NaN    NaN   NaN  NaN   NaN   \n",
      "4   1141.0  120.0    658.0  783880.0      3   NaN    NaN   NaN  NaN   NaN   \n",
      "\n",
      "   ...  debug01  cdata  bdata  bine7  opdw7  rema7  lo7c  samp7  welc7  musy7  \n",
      "0  ...      NaN    NaN    NaN    NaN    NaN    NaN   NaN    NaN    NaN    NaN  \n",
      "1  ...      NaN    NaN    NaN    NaN    NaN    NaN   NaN    NaN    NaN    NaN  \n",
      "2  ...      NaN    NaN    NaN    NaN    NaN    NaN   NaN    NaN    NaN    NaN  \n",
      "3  ...      NaN    NaN    NaN    NaN    NaN    NaN   NaN    NaN    NaN    NaN  \n",
      "4  ...      NaN    NaN    NaN    NaN    NaN    NaN   NaN    NaN    NaN    NaN  \n",
      "\n",
      "[5 rows x 184 columns]\n"
     ]
    }
   ],
   "source": [
    "FE = FeatureExtraction(trainLabel_path)\n",
    "\n",
    "data_section = FE.feature_extraction_section(source)\n",
    "\n",
    "data_section.to_csv('file_section.csv', encoding='utf-8', index=False)\n",
    "print (data_section.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      FF    F8     E8   A1    EB    E0    C3   DB    E9    FE  ...  \\\n",
      "0  376.0  41.0  154.0  9.0  34.0  12.0  44.0  5.0  14.0  39.0  ...   \n",
      "1    NaN   NaN    NaN  2.0   1.0   NaN   NaN  NaN   NaN   1.0  ...   \n",
      "2    1.0   NaN    NaN  NaN   NaN   NaN   NaN  NaN   NaN   NaN  ...   \n",
      "3    NaN   NaN    NaN  NaN   NaN   NaN   NaN  NaN   NaN   NaN  ...   \n",
      "4    NaN   NaN    1.0  NaN   NaN   1.0   1.0  NaN   NaN   2.0  ...   \n",
      "\n",
      "   TempFileName  DateStr  aGGGGjgGhgtg  aGGGGagvgGag  aGGGGG3gGpg  aGsg3g  \\\n",
      "0           NaN      NaN           NaN           NaN          NaN     NaN   \n",
      "1           NaN      NaN           NaN           NaN          NaN     NaN   \n",
      "2           NaN      NaN           NaN           NaN          NaN     NaN   \n",
      "3           NaN      NaN           NaN           NaN          NaN     NaN   \n",
      "4           NaN      NaN           NaN           NaN          NaN     NaN   \n",
      "\n",
      "   aGhg9g  aXx88  aMfcsubs  aRtm  \n",
      "0     NaN    NaN       NaN   NaN  \n",
      "1     NaN    NaN       NaN   NaN  \n",
      "2     NaN    NaN       NaN   NaN  \n",
      "3     NaN    NaN       NaN   NaN  \n",
      "4     NaN    NaN       NaN   NaN  \n",
      "\n",
      "[5 rows x 6668 columns]\n"
     ]
    }
   ],
   "source": [
    "FE = FeatureExtraction(trainLabel_path)\n",
    "\n",
    "data_opcode = FE.feature_extraction_opcode(source)\n",
    "\n",
    "data_opcode.to_csv('file_opcode.csv', encoding='utf-8', index=False)\n",
    "print (data_opcode.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"file_byte.csv\", sep=\",\", index_col=False)\n",
    "df2 = pd.read_csv(\"file_section.csv\", sep=\",\", index_col=False)\n",
    "\n",
    "if 'Class' in df1.columns:\n",
    "    print('true')\n",
    "    \n",
    "if 'Class' in df2.columns:\n",
    "    df2 = df2.drop('Class', axis=1)\n",
    "    \n",
    "\n",
    "\n",
    "new_df = pd.concat([df1.reset_index(drop=True), df2.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 441)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 =  pd.read_csv(\"file_opcode.csv\", sep=\",\", index_col=False)\n",
    "\n",
    "if 'Class' in df3.columns:\n",
    "    df3 = df3.drop('Class', axis=1)\n",
    "\n",
    "final_df = pd.concat([new_df.reset_index(drop=True), df3.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>97</th>\n",
       "      <th>FF</th>\n",
       "      <th>C6</th>\n",
       "      <th>49</th>\n",
       "      <th>C3</th>\n",
       "      <th>63</th>\n",
       "      <th>C0</th>\n",
       "      <th>74</th>\n",
       "      <th>59</th>\n",
       "      <th>...</th>\n",
       "      <th>TempFileName</th>\n",
       "      <th>DateStr</th>\n",
       "      <th>aGGGGjgGhgtg</th>\n",
       "      <th>aGGGGagvgGag</th>\n",
       "      <th>aGGGGG3gGpg</th>\n",
       "      <th>aGsg3g</th>\n",
       "      <th>aGhg9g</th>\n",
       "      <th>aXx88</th>\n",
       "      <th>aMfcsubs</th>\n",
       "      <th>aRtm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35208.0</td>\n",
       "      <td>1545.0</td>\n",
       "      <td>5996.0</td>\n",
       "      <td>508.0</td>\n",
       "      <td>873.0</td>\n",
       "      <td>741.0</td>\n",
       "      <td>383.0</td>\n",
       "      <td>1056.0</td>\n",
       "      <td>1803.0</td>\n",
       "      <td>958.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11674.0</td>\n",
       "      <td>3383.0</td>\n",
       "      <td>6864.0</td>\n",
       "      <td>3243.0</td>\n",
       "      <td>3185.0</td>\n",
       "      <td>3169.0</td>\n",
       "      <td>3417.0</td>\n",
       "      <td>3329.0</td>\n",
       "      <td>3402.0</td>\n",
       "      <td>3327.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21075.0</td>\n",
       "      <td>510.0</td>\n",
       "      <td>8366.0</td>\n",
       "      <td>586.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>669.0</td>\n",
       "      <td>430.0</td>\n",
       "      <td>1153.0</td>\n",
       "      <td>1843.0</td>\n",
       "      <td>506.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21689.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>8769.0</td>\n",
       "      <td>1962.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>2224.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>1820.0</td>\n",
       "      <td>925.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5743.0</td>\n",
       "      <td>3022.0</td>\n",
       "      <td>3163.0</td>\n",
       "      <td>3006.0</td>\n",
       "      <td>2973.0</td>\n",
       "      <td>3144.0</td>\n",
       "      <td>2977.0</td>\n",
       "      <td>3031.0</td>\n",
       "      <td>3053.0</td>\n",
       "      <td>3081.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>88264.0</td>\n",
       "      <td>342.0</td>\n",
       "      <td>2701.0</td>\n",
       "      <td>510.0</td>\n",
       "      <td>451.0</td>\n",
       "      <td>509.0</td>\n",
       "      <td>624.0</td>\n",
       "      <td>918.0</td>\n",
       "      <td>1441.0</td>\n",
       "      <td>407.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7539.0</td>\n",
       "      <td>3166.0</td>\n",
       "      <td>5020.0</td>\n",
       "      <td>3101.0</td>\n",
       "      <td>3171.0</td>\n",
       "      <td>3180.0</td>\n",
       "      <td>3058.0</td>\n",
       "      <td>3096.0</td>\n",
       "      <td>3140.0</td>\n",
       "      <td>3130.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11409.0</td>\n",
       "      <td>3208.0</td>\n",
       "      <td>6766.0</td>\n",
       "      <td>3120.0</td>\n",
       "      <td>3298.0</td>\n",
       "      <td>3208.0</td>\n",
       "      <td>3410.0</td>\n",
       "      <td>3262.0</td>\n",
       "      <td>3335.0</td>\n",
       "      <td>3263.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11522.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>7120.0</td>\n",
       "      <td>3138.0</td>\n",
       "      <td>3232.0</td>\n",
       "      <td>3281.0</td>\n",
       "      <td>3255.0</td>\n",
       "      <td>3314.0</td>\n",
       "      <td>3282.0</td>\n",
       "      <td>3213.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>71800.0</td>\n",
       "      <td>1692.0</td>\n",
       "      <td>4917.0</td>\n",
       "      <td>1613.0</td>\n",
       "      <td>1883.0</td>\n",
       "      <td>1723.0</td>\n",
       "      <td>3627.0</td>\n",
       "      <td>1981.0</td>\n",
       "      <td>5120.0</td>\n",
       "      <td>2044.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 7108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        00      97      FF      C6      49      C3      63      C0      74  \\\n",
       "0  35208.0  1545.0  5996.0   508.0   873.0   741.0   383.0  1056.0  1803.0   \n",
       "1  11674.0  3383.0  6864.0  3243.0  3185.0  3169.0  3417.0  3329.0  3402.0   \n",
       "2  21075.0   510.0  8366.0   586.0   689.0   669.0   430.0  1153.0  1843.0   \n",
       "3  21689.0    63.0  8769.0  1962.0   305.0  2224.0   432.0  1080.0  1820.0   \n",
       "4   5743.0  3022.0  3163.0  3006.0  2973.0  3144.0  2977.0  3031.0  3053.0   \n",
       "5  88264.0   342.0  2701.0   510.0   451.0   509.0   624.0   918.0  1441.0   \n",
       "6   7539.0  3166.0  5020.0  3101.0  3171.0  3180.0  3058.0  3096.0  3140.0   \n",
       "7  11409.0  3208.0  6766.0  3120.0  3298.0  3208.0  3410.0  3262.0  3335.0   \n",
       "8  11522.0  3250.0  7120.0  3138.0  3232.0  3281.0  3255.0  3314.0  3282.0   \n",
       "9  71800.0  1692.0  4917.0  1613.0  1883.0  1723.0  3627.0  1981.0  5120.0   \n",
       "\n",
       "       59  ...  TempFileName  DateStr  aGGGGjgGhgtg  aGGGGagvgGag  \\\n",
       "0   958.0  ...           NaN      NaN           NaN           NaN   \n",
       "1  3327.0  ...           NaN      NaN           NaN           NaN   \n",
       "2   506.0  ...           NaN      NaN           NaN           NaN   \n",
       "3   925.0  ...           NaN      NaN           NaN           NaN   \n",
       "4  3081.0  ...           NaN      NaN           NaN           NaN   \n",
       "5   407.0  ...           NaN      NaN           NaN           NaN   \n",
       "6  3130.0  ...           NaN      NaN           NaN           NaN   \n",
       "7  3263.0  ...           NaN      NaN           NaN           NaN   \n",
       "8  3213.0  ...           NaN      NaN           NaN           NaN   \n",
       "9  2044.0  ...           NaN      NaN           NaN           NaN   \n",
       "\n",
       "   aGGGGG3gGpg  aGsg3g  aGhg9g  aXx88  aMfcsubs  aRtm  \n",
       "0          NaN     NaN     NaN    NaN       NaN   NaN  \n",
       "1          NaN     NaN     NaN    NaN       NaN   NaN  \n",
       "2          NaN     NaN     NaN    NaN       NaN   NaN  \n",
       "3          NaN     NaN     NaN    NaN       NaN   NaN  \n",
       "4          NaN     NaN     NaN    NaN       NaN   NaN  \n",
       "5          NaN     NaN     NaN    NaN       NaN   NaN  \n",
       "6          NaN     NaN     NaN    NaN       NaN   NaN  \n",
       "7          NaN     NaN     NaN    NaN       NaN   NaN  \n",
       "8          NaN     NaN     NaN    NaN       NaN   NaN  \n",
       "9          NaN     NaN     NaN    NaN       NaN   NaN  \n",
       "\n",
       "[10 rows x 7108 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head(10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "id": "j2wGOaIeObpO",
    "outputId": "a987d38b-527d-451f-8eea-9db30077ebae"
   },
   "source": [
    "\n",
    "print('=========================================================================================================================')\n",
    "print('RUNNING FOR BYTES')\n",
    "metrics_list_bytes = []\n",
    "model_labels=[]\n",
    "\n",
    "#FE = FeatureExtraction(trainLabel_path)\n",
    "#data = FE.feature_extraction_bytes(MalwaDataPath)\n",
    "# data = FE.feature_extraction_opcode('MalwareData')\n",
    "# data = FE.feature_extraction_section('MalwareData')\n",
    "data = final_df\n",
    "print(data.head())\n",
    "\n",
    "\n",
    "# X -> features, y -> label\n",
    "X = data.loc[:, data.columns != 'Class']\n",
    "X = np.array(X)\n",
    "print('is nan', np.any(np.isnan(X)))\n",
    "X = np.nan_to_num(X)\n",
    "print('is nan', np.any(np.isnan(X)))\n",
    "\n",
    "y = data['Class']\n",
    "\n",
    "\n",
    "# dividing X, y into train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "\n",
    "\n",
    "my_models = Model()\n",
    "# accuracy, precision, recall, fscore, fpr = my_models.decisionTree(X_train, y_train, X_test, y_test)\n",
    "model_labels.append('DT')\n",
    "metrics_list_bytes.append(my_models.decisionTree(X_train, y_train, X_test, y_test))\n",
    "\n",
    "model_labels.append('SVC')\n",
    "metrics_list_bytes.append(my_models.SVClassifier(X_train, y_train, X_test, y_test))\n",
    "\n",
    "model_labels.append('KNN')\n",
    "metrics_list_bytes.append(my_models.KNearestClassifier(X_train, y_train, X_test, y_test))\n",
    "\n",
    "model_labels.append('NB')\n",
    "metrics_list_bytes.append(my_models.NaiveBayes(X_train, y_train, X_test, y_test))\n",
    "\n",
    "model_labels.append('SGD')\n",
    "metrics_list_bytes.append(my_models.SGD(X_train, y_train, X_test, y_test))\n",
    "\n",
    "model_labels.append('LR')\n",
    "metrics_list_bytes.append(my_models.LogisticReg(X_train, y_train, X_test, y_test))\n",
    "\n",
    "model_labels.append('RF')\n",
    "metrics_list_bytes.append(my_models.RandomForest(X_train, y_train, X_test, y_test))\n",
    "\n",
    "\n",
    "train_x, test_x, train_y, test_y, encoder = my_models.DnnPreprocessing(X, y)\n",
    "accuracy, precision, recall, fscore, fpr = my_models.DNN_Model(train_x, test_x, train_y, test_y, epoch=30, encoder=encoder)\n",
    "\n",
    "model_labels.append('DNN')\n",
    "metrics_list_bytes.append((accuracy, precision, recall, fscore, fpr))\n",
    "\n",
    "\n",
    "print('=================================================================================================================================')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cIqBHjgBmSTP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print('=========================================================================================================================')\n",
    "print('RUNNING FOR OPCODES')\n",
    "metrics_list_opcodes = []\n",
    "model_labels=[]\n",
    "\n",
    "FE = FeatureExtraction('trainLabels.csv')\n",
    "# data = FE.feature_extraction_bytes(MalwaDataPath)\n",
    "data = FE.feature_extraction_opcode(MalwaDataPath)\n",
    "# data = FE.feature_extraction_section('MalwareData')\n",
    "print(data.head())\n",
    "\n",
    "\n",
    "# X -> features, y -> label\n",
    "X = data.loc[:, data.columns != 'Class']\n",
    "X = np.array(X)\n",
    "print('is nan', np.any(np.isnan(X)))\n",
    "X = np.nan_to_num(X)\n",
    "print('is nan', np.any(np.isnan(X)))\n",
    "\n",
    "y = data['Class']\n",
    "\n",
    "\n",
    "# dividing X, y into train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "\n",
    "\n",
    "my_models = Model()\n",
    "# accuracy, precision, recall, fscore, fpr = my_models.decisionTree(X_train, y_train, X_test, y_test)\n",
    "model_labels.append('DT')\n",
    "metrics_list_opcodes.append(my_models.decisionTree(X_train, y_train, X_test, y_test))\n",
    "\n",
    "model_labels.append('SVC')\n",
    "metrics_list_opcodes.append(my_models.SVClassifier(X_train, y_train, X_test, y_test))\n",
    "\n",
    "model_labels.append('KNN')\n",
    "metrics_list_opcodes.append(my_models.KNearestClassifier(X_train, y_train, X_test, y_test))\n",
    "\n",
    "model_labels.append('NB')\n",
    "metrics_list_opcodes.append(my_models.NaiveBayes(X_train, y_train, X_test, y_test))\n",
    "\n",
    "model_labels.append('SGD')\n",
    "metrics_list_opcodes.append(my_models.SGD(X_train, y_train, X_test, y_test))\n",
    "\n",
    "model_labels.append('LR')\n",
    "metrics_list_opcodes.append(my_models.LogisticReg(X_train, y_train, X_test, y_test))\n",
    "\n",
    "model_labels.append('RF')\n",
    "metrics_list_opcodes.append(my_models.RandomForest(X_train, y_train, X_test, y_test))\n",
    "\n",
    "\n",
    "train_x, test_x, train_y, test_y, encoder = my_models.DnnPreprocessing(X, y)\n",
    "accuracy, precision, recall, fscore, fpr = my_models.DNN_Model(train_x, test_x, train_y, test_y, epoch=100, encoder=encoder)\n",
    "\n",
    "model_labels.append('DNN')\n",
    "metrics_list_opcodes.append((accuracy, precision, recall, fscore, fpr))\n",
    "\n",
    "\n",
    "print('=================================================================================================================================')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jnXeN99bvnje"
   },
   "outputs": [],
   "source": [
    "\n",
    "print('=========================================================================================================================')\n",
    "print('RUNNING FOR SECTION')\n",
    "metrics_list_section = []\n",
    "model_labels=[]\n",
    "\n",
    "FE = FeatureExtraction('trainLabels.csv')\n",
    "# data = FE.feature_extraction_bytes(MalwaDataPath)\n",
    "# data = FE.feature_extraction_opcode(MalwaDataPath)\n",
    "data = FE.feature_extraction_section(MalwaDataPath)\n",
    "print(data.head())\n",
    "\n",
    "\n",
    "# X -> features, y -> label\n",
    "X = data.loc[:, data.columns != 'Class']\n",
    "X = np.array(X)\n",
    "print('is nan', np.any(np.isnan(X)))\n",
    "X = np.nan_to_num(X)\n",
    "print('is nan', np.any(np.isnan(X)))\n",
    "\n",
    "y = data['Class']\n",
    "\n",
    "\n",
    "# dividing X, y into train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "\n",
    "\n",
    "my_models = Model()\n",
    "# accuracy, precision, recall, fscore, fpr = my_models.decisionTree(X_train, y_train, X_test, y_test)\n",
    "model_labels.append('DT')\n",
    "metrics_list_section.append(my_models.decisionTree(X_train, y_train, X_test, y_test))\n",
    "\n",
    "model_labels.append('SVC')\n",
    "metrics_list_section.append(my_models.SVClassifier(X_train, y_train, X_test, y_test))\n",
    "\n",
    "model_labels.append('KNN')\n",
    "metrics_list_section.append(my_models.KNearestClassifier(X_train, y_train, X_test, y_test))\n",
    "\n",
    "model_labels.append('NB')\n",
    "metrics_list_section.append(my_models.NaiveBayes(X_train, y_train, X_test, y_test))\n",
    "\n",
    "model_labels.append('SGD')\n",
    "metrics_list_section.append(my_models.SGD(X_train, y_train, X_test, y_test))\n",
    "\n",
    "model_labels.append('LR')\n",
    "metrics_list_section.append(my_models.LogisticReg(X_train, y_train, X_test, y_test))\n",
    "\n",
    "model_labels.append('RF')\n",
    "metrics_list_section.append(my_models.RandomForest(X_train, y_train, X_test, y_test))\n",
    "\n",
    "train_x, test_x, train_y, test_y, encoder = my_models.DnnPreprocessing(X, y)\n",
    "accuracy, precision, recall, fscore, fpr = my_models.DNN_Model(train_x, test_x, train_y, test_y, epoch=100, encoder=encoder)\n",
    "\n",
    "model_labels.append('DNN')\n",
    "metrics_list_section.append((accuracy, precision, recall, fscore, fpr))\n",
    "\n",
    "\n",
    "print('=================================================================================================================================')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2web-Hs2dyJ"
   },
   "source": [
    "**Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OOEcfH9dmQvy"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Analysis\n",
    "print('ANALYSIS')\n",
    "accuracies = [a[0] for a in metrics_list_bytes]\n",
    "accuracies2 = [a[0] for a in metrics_list_opcodes]\n",
    "accuracies3 = [a[0] for a in metrics_list_section]\n",
    "plt.plot(model_labels, accuracies, '-o', label='bytecodes')\n",
    "plt.plot(model_labels, accuracies2, '-o', label='opcodes')\n",
    "plt.plot(model_labels, accuracies3, '-o', label='section')\n",
    "plt.legend()\n",
    "plt.title('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "tpr = [a[2] for a in metrics_list_bytes]\n",
    "fpr = [a[4] for a in metrics_list_bytes]\n",
    "x = np.arange(len(model_labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, tpr, width, label='tpr_byte')\n",
    "rects2 = ax.bar(x + width/2, fpr, width, label='fpr_byte')\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "# ax.set_ylabel('Scores')\n",
    "ax.set_title('TPR and FPR bytes')\n",
    "# ax.set_xticks(x, model_labels)\n",
    "ax.legend()\n",
    "ax.bar_label(rects1, padding=3)\n",
    "ax.bar_label(rects2, padding=3)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "tpr = [a[2] for a in metrics_list_opcodes]\n",
    "fpr = [a[4] for a in metrics_list_opcodes]\n",
    "x = np.arange(len(model_labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, tpr, width, label='tpr_opcode')\n",
    "rects2 = ax.bar(x + width/2, fpr, width, label='fpr_opcode')\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "# ax.set_ylabel('Scores')\n",
    "ax.set_title('TPR and FPR opcodes')\n",
    "# ax.set_xticks(x, model_labels)\n",
    "ax.legend()\n",
    "ax.bar_label(rects1, padding=3)\n",
    "ax.bar_label(rects2, padding=3)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "tpr = [a[2] for a in metrics_list_section]\n",
    "fpr = [a[4] for a in metrics_list_section]\n",
    "x = np.arange(len(model_labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, tpr, width, label='tpr_section')\n",
    "rects2 = ax.bar(x + width/2, fpr, width, label='fpr_section')\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "# ax.set_ylabel('Scores')\n",
    "ax.set_title('TPR and FPR section')\n",
    "# ax.set_xticks(x, model_labels)\n",
    "ax.legend()\n",
    "ax.bar_label(rects1, padding=3)\n",
    "ax.bar_label(rects2, padding=3)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Malware_analysis_Using_ML_DL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
