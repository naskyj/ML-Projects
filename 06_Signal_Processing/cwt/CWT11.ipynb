{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TtoMPNIKv0u-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import skimage.io\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import argparse\n",
    "import pywt\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.fftpack import fft\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.patches as patches\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras.preprocessing import image as image_utils\n",
    "from keras.applications.imagenet_utils import preprocess_input, decode_predictions\n",
    "from keras.layers import Convolution2D, Dense, Input, Flatten, Dropout, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D, Activation, Concatenate\n",
    "from keras import Sequential\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "#from keras.optimizers import SGD,Adam\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "#Different Transfer learning rate/models\n",
    "#https://keras.io/api/applications/\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "# from keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.resnet import ResNet101\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg19 import VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v5vHjlm8v4AV"
   },
   "outputs": [],
   "source": [
    "#train dataset\n",
    "df_train = pd.read_excel(\"gasoline.xlsx\", header=None) #trainning data\n",
    "df_train_label = pd.read_excel(\"gasoline.xlsx\", sheet_name=1, header=None)\n",
    "\n",
    "\n",
    "#data cleaning: remove redundant first column\n",
    "df_train_label = df_train_label.iloc[: , 1:]\n",
    "\n",
    "df_train_label.columns =['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V1GAUCZ8xY4H",
    "outputId": "4e2ad016-d0a7-4e9c-b45c-468334233da7"
   },
   "outputs": [],
   "source": [
    "#check shapes and data of dataframes\n",
    "print(df_train.shape)\n",
    "print(df_train_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_label.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wavelet(signal, path, scales = np.arange(0.25, 512),\n",
    "\n",
    "                    waveletname = 'morl', #'cmor',\n",
    "\n",
    "                   cmap = plt.cm.seismic,\n",
    "\n",
    "                   title = 'Wavelet Transform (Power Spectrum) of signal',\n",
    "\n",
    "                   ylabel = 'Scales',\n",
    "\n",
    "                   xlabel = 'Time'):\n",
    "    \n",
    "    \n",
    "    #decode the signal\n",
    "    \n",
    "    time = np.arange(len(signal))\n",
    "    \n",
    "    dt = time[1] - time[0]\n",
    "\n",
    "    [coefficients, frequencies] = pywt.cwt(signal, scales, waveletname, dt)\n",
    "\n",
    "    power = (abs(coefficients)) ** 2\n",
    "\n",
    "   \n",
    "\n",
    "    period = 1. / frequencies\n",
    "\n",
    "    maxVal = power.max()\n",
    "    \n",
    "    n = 8\n",
    "\n",
    "    levels = [maxVal/(2**(n-i)) for i in range(n)]\n",
    "\n",
    "    contourlevels = np.log2(levels)\n",
    "\n",
    "    #print(power.min(), power.max())\n",
    "    \n",
    "   \n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(2.24, 2.24), dpi=200)\n",
    "\n",
    "    im = ax.contourf(time, np.log2(period), np.log2(power), contourlevels, extend='both',cmap=cmap)\n",
    "\n",
    "   \n",
    "\n",
    " \n",
    "\n",
    "    #ax.set_title(title, fontsize=20)\n",
    "\n",
    "    #ax.set_ylabel(ylabel, fontsize=18)\n",
    "\n",
    "    #ax.set_xlabel(xlabel, fontsize=18)\n",
    "\n",
    "   \n",
    "\n",
    "   # yticks = 2**np.arange(np.ceil(np.log2(period.min())), np.ceil(np.log2(period.max())))\n",
    "\n",
    "    #ax.set_yticks(np.log2(yticks))\n",
    "\n",
    "    #ax.set_yticklabels(yticks)\n",
    "\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    #ax.set_ylim(ylim[0], -1)\n",
    "\n",
    "   \n",
    "\n",
    "    #cbar_ax = fig.add_axes([0.95, 0.5, 0.03, 0.25])\n",
    "\n",
    "    #fig.colorbar(im, cax=cbar_ax, orientation=\"vertical\")\n",
    "    \n",
    "    \n",
    "    plt.ioff()\n",
    "   \n",
    "    plt.axis('off')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.savefig(path,  bbox_inches='tight',pad_inches = 0)\n",
    "    \n",
    "\n",
    "    #plt.savefig(\"test.png\",  bbox_inches='tight',pad_inches = 0)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3rMLYY0b6p1y"
   },
   "outputs": [],
   "source": [
    "train_path = 'scalograms/training/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_validation_label.columns =['Class']\n",
    "ids = []\n",
    "names = []\n",
    "\n",
    "for row in df_train.itertuples():\n",
    "    \n",
    "    image_id = 'image' + str(row.Index)\n",
    "    name = 'image' + str(row.Index) + '.png'\n",
    "\n",
    "    ids.append(image_id)\n",
    "    names.append(name)  \n",
    "        \n",
    "        \n",
    "        \n",
    "    filepath = train_path + name\n",
    "        \n",
    "    plot_wavelet(df_train.iloc[row.Index], filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YDiBHp96VX_9"
   },
   "outputs": [],
   "source": [
    "img_width = 347\n",
    "img_height = 338"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NuM6vaTh1iod"
   },
   "outputs": [],
   "source": [
    "def extract_scalogram(path):\n",
    "    height =  img_height# pixels in length\n",
    "    width = img_width # pixels in width\n",
    "  \n",
    "    imgs = np.empty((0, width, height, 3)) \n",
    "\n",
    "    for filename in os.listdir(path):\n",
    "        \n",
    "        if filename.endswith(\".png\"):\n",
    "\n",
    "            img = Image.open(os.path.join(path, filename)).convert('RGB')\n",
    "\n",
    "            imgs = np.append(imgs, np.array(img).reshape((1, width, height, 3)), axis=0)\n",
    "    \n",
    "    return imgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4DTNQ8Co_ghr"
   },
   "outputs": [],
   "source": [
    "X_train = extract_scalogram(train_path)\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "df_train_label['Class_int'] = pd.Categorical(df_train_label['Class']).codes\n",
    "\n",
    "y_train = df_train_label['Class_int']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hqrIQWmDDx5x"
   },
   "outputs": [],
   "source": [
    "y_train=to_categorical(y_train)\n",
    "\n",
    "print((X_train.shape,y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cawLUYxo9x1T"
   },
   "outputs": [],
   "source": [
    "def normalize_negative_one(img):\n",
    "    normalized_input = (img - np.amin(img)) / (np.amax(img) - np.amin(img))\n",
    "    return 2*normalized_input - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "icmv_rTs91mu"
   },
   "outputs": [],
   "source": [
    "X_train = normalize_negative_one(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
    "    test_size=0.20, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train: ', X_train.shape)\n",
    "print('y_train: ', y_train.shape)\n",
    "print('X_val: ', X_val.shape)\n",
    "print('y_val: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G9zr0H7_NdTV"
   },
   "outputs": [],
   "source": [
    "#Initializing the hyperparameters\n",
    "batch_size= 16\n",
    "epochs=10\n",
    "learn_rate=.001\n",
    "sgd=SGD(lr=learn_rate,momentum=.9,nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_-EI_fgGEOIP"
   },
   "outputs": [],
   "source": [
    "#Image Data Augmentation -  technique of altering the existing data to create some more data for the model training proces\n",
    "\n",
    "train_dategen = ImageDataGenerator(rescale = 1./255.,rotation_range = 40, width_shift_range = 0.2, height_shift_range = 0.2, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)\n",
    "train_generator = train_dategen.flow(X_train,  y_train, batch_size= batch_size)\n",
    "\n",
    "validation_dategen = ImageDataGenerator(rescale = 1./255.,rotation_range = 40, width_shift_range = 0.2, height_shift_range = 0.2, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)\n",
    "validation_generator = validation_dategen.flow(X_val, y_val, batch_size= batch_size)\n",
    "\n",
    "test_generator = ImageDataGenerator(rescale = 1./255.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2D9Z61UHNXAF"
   },
   "outputs": [],
   "source": [
    "#Learning Rate Annealer\n",
    "lrr= ReduceLROnPlateau(monitor='val_accuracy', factor=.01, patience=3, min_lr=1e-5) #change learning rATE  to get the best result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2W6cNDTONZ5j"
   },
   "outputs": [],
   "source": [
    "#Defining the googlenet model\n",
    "#use resnet - try various versions of resnet\n",
    "#compare googlenet to resnet and the other 4, to see which one is better. use the latest version of them\n",
    "\n",
    "#base_model = InceptionV3(include_top = False, weights = 'imagenet', input_shape = (img_width,img_height,3))\n",
    "# base_model = ResNet50(include_top = False, weights = 'imagenet', input_shape = (img_width,img_height,3)) #\n",
    "# base_model = ResNet101(include_top = False, weights = 'imagenet', input_shape = (img_width,img_height,3))\n",
    "base_model = Xception(include_top = False, weights = 'imagenet', input_shape = (img_width,img_height,3))\n",
    "# base_model = VGG16(include_top = False, weights = 'imagenet', input_shape = (img_width,img_height,3))\n",
    "# base_model = VGG19(include_top = False, weights = 'imagenet', input_shape = (img_width,img_height,3))\n",
    "\n",
    "\n",
    "#use resnet - try various versions of resnet\n",
    "#compare googlenet to resnet and the other 4, to see which one is better. use the latest version of them\n",
    "\n",
    "#changing the last layer\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "#https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a\n",
    "\n",
    "# #changing the last layer\n",
    "# for layer in base_model.layers:\n",
    "#   layer.trainable = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zURWTWz7MAyU"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_YfCWpb4ZSK2"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lRMSALYswHVJ"
   },
   "outputs": [],
   "source": [
    "model.fit_generator(train_generator, \n",
    "                    epochs = epochs, \n",
    "                    steps_per_epoch = X_train.shape[0]//batch_size, \n",
    "                    validation_data = validation_generator, \n",
    "                    validation_steps = X_val.shape[0]//batch_size, callbacks=[lrr],  verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SSiqE6RyFjV7"
   },
   "outputs": [],
   "source": [
    "#Plotting the training and validation loss and accuracy\n",
    "f,ax=plt.subplots(2,1) \n",
    "\n",
    "#Loss\n",
    "ax[0].plot(model.history.history['loss'],color='b',label='Training Loss')\n",
    "ax[0].plot(model.history.history['val_loss'],color='r',label='Validation Loss')\n",
    "\n",
    "#Accuracy\n",
    "ax[1].plot(model.history.history['accuracy'],color='b',label='Training  Accuracy')\n",
    "ax[1].plot(model.history.history['val_accuracy'],color='r',label='Validation Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing with regular\n",
    "df_regular = pd.read_excel(\"regular.xlsx\", header=None)\n",
    "df_regular_label = pd.read_excel(\"regular.xlsx\", sheet_name=1, header=None)\n",
    "\n",
    "df_regular_label = df_regular_label.iloc[0:15 , 0:1]\n",
    "df_regular_label.columns =['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_regular.shape)\n",
    "print(df_regular_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_path = 'scalograms/verification/regular/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "names = []\n",
    "\n",
    "for row in df_regular.itertuples():\n",
    "    \n",
    "    image_id = 'image' + str(row.Index)\n",
    "    name = 'image' + str(row.Index) + '.png'\n",
    "\n",
    "    ids.append(image_id)\n",
    "    names.append(name)  \n",
    "        \n",
    "        \n",
    "        \n",
    "    filepath = regular_path + name\n",
    "        \n",
    "    plot_wavelet(df_regular.iloc[row.Index], filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_regular = extract_scalogram(regular_path)\n",
    "\n",
    "\n",
    "df_regular_label['Class_int'] = pd.Categorical(df_regular_label['Class']).codes\n",
    "\n",
    "y_regular = df_regular_label['Class_int']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_regular=to_categorical(y_regular)\n",
    "\n",
    "print((X_regular.shape,y_regular.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_regular = normalize_negative_one(X_regular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11rtDrj_FjIn"
   },
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict(X_regular), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XgpiOLlg3vC1"
   },
   "outputs": [],
   "source": [
    "print(y_pred) #predict what class the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing with midgrade\n",
    "df_midgrade = pd.read_excel(\"midgrade.xlsx\", header=None)\n",
    "df_midgrade_label = pd.read_excel(\"midgrade.xlsx\", sheet_name=1, header=None)\n",
    "\n",
    "df_midgrade_label = df_midgrade_label.iloc[0:15 , 0:1]\n",
    "df_midgrade_label.columns =['Class']\n",
    "\n",
    "print(df_midgrade.shape)\n",
    "print(df_midgrade_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midgrade_path = 'scalograms/verification/midgrade/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "names = []\n",
    "\n",
    "for row in df_midgrade.itertuples():\n",
    "    \n",
    "    image_id = 'image' + str(row.Index)\n",
    "    name = 'image' + str(row.Index) + '.png'\n",
    "\n",
    "    ids.append(image_id)\n",
    "    names.append(name)  \n",
    "        \n",
    "        \n",
    "        \n",
    "    filepath = midgrade_path + name\n",
    "        \n",
    "    plot_wavelet(df_midgrade.iloc[row.Index], filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_midgrade = extract_scalogram(midgrade_path)\n",
    "\n",
    "\n",
    "df_midgrade_label['Class_int'] = pd.Categorical(df_midgrade_label['Class']).codes\n",
    "\n",
    "y_midgrade = df_midgrade_label['Class_int']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_midgrade=to_categorical(y_midgrade)\n",
    "\n",
    "print((X_midgrade.shape,y_midgrade.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_midgrade = normalize_negative_one(X_midgrade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict(X_midgrade), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing with premium\n",
    "df_premium = pd.read_excel(\"premium.xlsx\", header=None)\n",
    "df_premium_label = pd.read_excel(\"premium.xlsx\", sheet_name=1, header=None)\n",
    "\n",
    "df_premium_label = df_premium_label.iloc[0:15 , 0:1]\n",
    "df_premium_label.columns =['Class']\n",
    "\n",
    "print(df_premium.shape)\n",
    "print(df_premium_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "premium_path = 'scalograms/verification/premium/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "names = []\n",
    "\n",
    "for row in df_premium.itertuples():\n",
    "    \n",
    "    image_id = 'image' + str(row.Index)\n",
    "    name = 'image' + str(row.Index) + '.png'\n",
    "\n",
    "    ids.append(image_id)\n",
    "    names.append(name)  \n",
    "        \n",
    "        \n",
    "        \n",
    "    filepath = premium_path + name\n",
    "        \n",
    "    plot_wavelet(df_premium.iloc[row.Index], filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_premium = extract_scalogram(premium_path)\n",
    "\n",
    "\n",
    "df_premium_label['Class_int'] = pd.Categorical(df_premium_label['Class']).codes\n",
    "\n",
    "y_premium = df_premium_label['Class_int']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_premium=to_categorical(y_premium)\n",
    "\n",
    "print((X_premium.shape,y_premium.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_premium = normalize_negative_one(X_premium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict(X_premium), axis=-1)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weathered = pd.read_excel(\"weathered gasoline.xlsx\", header=None)\n",
    "df_weathered_label = pd.read_excel(\"weathered gasoline.xlsx\", sheet_name=1, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weathered_label = df_weathered_label.iloc[: , 0:2]\n",
    "df_weathered_label.columns =['Class', 'Values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.concat([df_weathered, df_weathered_label], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weathered_25 = df1[df1.Values == 0.25]\n",
    "\n",
    "df_weathered_50 = df1[df1.Values == 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weathered_25_label = df_weathered_25['Class']\n",
    "\n",
    "df_weathered_25.drop(['Class', 'Values'], axis=1, inplace=True)\n",
    "df_weathered_25.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weathered_50_label = df_weathered_50['Class']\n",
    "\n",
    "df_weathered_50.drop(['Class', 'Values'], axis=1, inplace=True)\n",
    "df_weathered_50.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weathered_25_path = 'scalograms/evaluation/25/'\n",
    "weathered_50_path = 'scalograms/evaluation/50/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "names = []\n",
    "\n",
    "for row in df_weathered_25.itertuples():\n",
    "    \n",
    "    image_id = 'image' + str(row.Index)\n",
    "    name = 'image' + str(row.Index) + '.png'\n",
    "\n",
    "    ids.append(image_id)\n",
    "    names.append(name)  \n",
    "        \n",
    "        \n",
    "        \n",
    "    filepath = weathered_25_path + name\n",
    "        \n",
    "    plot_wavelet(df_weathered_25.iloc[row.Index], filepath)\n",
    "    \n",
    "for row in df_weathered_50.itertuples():\n",
    "    \n",
    "    image_id = 'image' + str(row.Index)\n",
    "    name = 'image' + str(row.Index) + '.png'\n",
    "\n",
    "    ids.append(image_id)\n",
    "    names.append(name)  \n",
    "        \n",
    "        \n",
    "        \n",
    "    filepath = weathered_50_path + name\n",
    "        \n",
    "    plot_wavelet(df_weathered_50.iloc[row.Index], filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_weathered_25 = extract_scalogram(weathered_25_path)\n",
    "X_weathered_25 = normalize_negative_one(X_weathered_25)\n",
    "\n",
    "\n",
    "df_weathered_25_label['Class_int'] = pd.Categorical(df_weathered_25_label['Class']).codes\n",
    "y_weathered_25 = df_weathered_25_label['Class_int']\n",
    "y_weathered_25 = to_categorical(y_weathered_25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict(X_weathered_25), axis=-1)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_weathered_50 = extract_scalogram(weathered_50_path)\n",
    "X_weathered_50 = normalize_negative_one(X_weathered_50)\n",
    "\n",
    "\n",
    "df_weathered_50_label['Class_int'] = pd.Categorical(df_weathered_50_label['Class']).codes\n",
    "y_weathered_50 = df_weathered_50_label['Class_int']\n",
    "y_weathered_50 = to_categorical(y_weathered_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict(X_weathered_50), axis=-1)\n",
    "print(y_pred)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Untitled6.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
